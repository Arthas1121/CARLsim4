/* * Copyright (c) 2015 Regents of the University of California. All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 *
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 *
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * 3. The names of its contributors may not be used to endorse or promote
 *    products derived from this software without specific prior written
 *    permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
 * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
 * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
 * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
 * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
 * LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 * NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 * *********************************************************************************************** *
 * CARLsim
 * created by: (MDR) Micah Richert, (JN) Jayram M. Nageswaran
 * maintained by:
 * (MA) Mike Avery <averym@uci.edu>
 * (MB) Michael Beyeler <mbeyeler@uci.edu>,
 * (KDC) Kristofor Carlson <kdcarlso@uci.edu>
 * (TSC) Ting-Shuo Chou <tingshuc@uci.edu>
 *
 * CARLsim available from http://socsci.uci.edu/~jkrichma/CARLsim/
 * Ver 5/22/2015
 */

#include <snn.h>

#include <spike_buffer.h>

void SNN::doCPUSim() {
	// decay STP vars and conductances
	doSTPUpdateAndDecayCond();

	spikeGeneratorUpdate();

	// find the neurons that has fired..
	findFiring();

	updateTimingTable();

	routeSpikes();

	doCurrentUpdate();

	globalStateUpdate();

	clearExtFiringTable();

	return;
}

// spikeGeneratorUpdate on CPUs
void SNN::spikeGeneratorUpdate() {
	// If poisson rate has been updated, assign new poisson rate
	if (spikeRateUpdated) {
		assignPoissonFiringRate();
		spikeRateUpdated = false;
	}

	// If time slice has expired, check if new spikes needs to be generated by user-defined spike generators
	generateUserDefinedSpikes();

	for (int netId = 0; netId < MAX_NET_PER_SNN; netId++) {
		if (!groupPartitionLists[netId].empty()) {
			assert(cpuRuntimeData[netId].allocated);

			// update the random number for poisson spike generator (spikes generated by rate)
			for (int poisN = 0; poisN < networkConfigs[netId].numNPois; poisN++) {
				// set CPU_MODE Random Gen, store random number to g(c)puRandNums
				cpuRuntimeData[netId].randNum[poisN] = drand48();
			}

			// Use spike generators (user-defined callback function)
			if (networkConfigs[netId].numNSpikeGen > 0) {
				assert(managerRuntimeData.spikeGenBits != NULL);

				// reset the bit status of the spikeGenBits...
				memset(managerRuntimeData.spikeGenBits, 0, sizeof(int) * (networkConfigs[netId].numNSpikeGen / 32 + 1));

				// fill spikeGenBits from SpikeBuffer
				fillSpikeGenBits(netId);

				// copy the spikeGenBits from the CPU to the GPU..
				memcpy(cpuRuntimeData[netId].spikeGenBits, managerRuntimeData.spikeGenBits, sizeof(int) * (networkConfigs[netId].numNSpikeGen / 32 + 1));
			}
		}
	}
}

void SNN::updateTimingTable() {
	for (int netId = 0; netId < MAX_NET_PER_SNN; netId++) {
		if (!groupPartitionLists[netId].empty()) {
			cpuRuntimeData[netId].timeTableD2[simTimeMs+glbNetworkConfig.maxDelay+1] = cpuRuntimeData[netId].spikeCountD2Sec;
			cpuRuntimeData[netId].timeTableD1[simTimeMs+glbNetworkConfig.maxDelay+1] = cpuRuntimeData[netId].spikeCountD1Sec;
		}
	}
}

void SNN::routeSpikes() {
}

void SNN::clearExtFiringTable() {
}

void SNN::doCurrentUpdate() {
	for (int netId = 0; netId < MAX_NET_PER_SNN; netId++) {
		if (!groupPartitionLists[netId].empty()) {
			doCurrentUpdateD2(netId);
		}
	}

	for (int netId = 0; netId < MAX_NET_PER_SNN; netId++) {
		if (!groupPartitionLists[netId].empty()) {
			doCurrentUpdateD1(netId);
		}
	}
}

// This method loops through all spikes that are generated by neurons with a delay of 1ms
// and delivers the spikes to the appropriate post-synaptic neuron
void SNN::doCurrentUpdateD1(int netId) {
	int k     = cpuRuntimeData[netId].spikeCountD1Sec - 1;
	int k_end = cpuRuntimeData[netId].timeTableD1[simTimeMs + glbNetworkConfig.maxDelay];

	while((k >= k_end) && (k >= 0)) {
		int lNId = cpuRuntimeData[netId].firingTableD1[k];
		assert(lNId < networkConfigs[netId].numN);

		DelayInfo dPar = cpuRuntimeData[netId].postDelayInfo[lNId * (glbNetworkConfig.maxDelay + 1)];

		unsigned int offset = cpuRuntimeData[netId].cumulativePost[lNId];

		for(int idx_d = dPar.delay_index_start; idx_d < (dPar.delay_index_start + dPar.delay_length); idx_d = idx_d + 1) {
				generatePostSynapticSpike(lNId, idx_d, offset, 0);
		}

		k = k - 1;
	}
}

// This method loops through all spikes that are generated by neurons with a delay of 2+ms
// and delivers the spikes to the appropriate post-synaptic neuron
void SNN::doCurrentUpdateD2(int netId) {
	int k = cpuRuntimeData[netId].spikeCountD2Sec - 1;
	int k_end = cpuRuntimeData[netId].timeTableD2[simTimeMs + 1];
	int t_pos = simTimeMs;

	while((k >= k_end) && (k >= 0)) {
		// get the neuron id from the index k
		int lNId  = cpuRuntimeData[netId].firingTableD2[k];

		// find the time of firing from the timeTable using index k
		while (!((k >= cpuRuntimeData[netId].timeTableD2[t_pos + glbNetworkConfig.maxDelay]) && (k < cpuRuntimeData[netId].timeTableD2[t_pos + glbNetworkConfig.maxDelay + 1]))) {
			t_pos = t_pos - 1;
			assert((t_pos + glbNetworkConfig.maxDelay - 1) >= 0);
		}

		// \TODO: Instead of using the complex timeTable, can neuronFiringTime value...???
		// Calculate the time difference between time of firing of neuron and the current time...
		int tD = simTimeMs - t_pos;

		assert((tD < glbNetworkConfig.maxDelay) && (tD >= 0));
		assert(lNId < networkConfigs[netId].numN);

		DelayInfo dPar = cpuRuntimeData[netId].postDelayInfo[lNId * (glbNetworkConfig.maxDelay  +1) + tD];

		unsigned int offset = cpuRuntimeData[netId].cumulativePost[lNId];

		// for each delay variables
		for(int idx_d = dPar.delay_index_start; idx_d < (dPar.delay_index_start + dPar.delay_length); idx_d = idx_d + 1) {
			generatePostSynapticSpike(lNId, idx_d, offset, tD);
		}

		k = k - 1;
	}
}

void SNN::doSTPUpdateAndDecayCond() {
	// ToDo: This can be further optimized using multiple threads allocated on mulitple CPU cores
	for (int netId = 0; netId < MAX_NET_PER_SNN; netId++) {
		if (!groupPartitionLists[netId].empty()) {
			//decay the STP variables before adding new spikes.
			for (int lGrpId = 0; lGrpId < networkConfigs[netId].numGroups; lGrpId++) {
				for(int lNId = groupConfigs[netId][lGrpId].lStartN; lNId <= groupConfigs[netId][lGrpId].lEndN; lNId++) {
					if (groupConfigs[netId][lGrpId].WithSTP) {
						int ind_plus  = STP_BUF_POS(lNId, simTime, glbNetworkConfig.maxDelay);
						int ind_minus = STP_BUF_POS(lNId, (simTime - 1), glbNetworkConfig.maxDelay);
						cpuRuntimeData[netId].stpu[ind_plus] = cpuRuntimeData[netId].stpu[ind_minus] * (1.0f - groupConfigs[netId][lGrpId].STP_tau_u_inv);
						cpuRuntimeData[netId].stpx[ind_plus] = cpuRuntimeData[netId].stpx[ind_minus] + (1.0f - cpuRuntimeData[netId].stpx[ind_minus]) * groupConfigs[netId][lGrpId].STP_tau_x_inv;
					}

					// decay conductances
					if (networkConfigs[netId].sim_with_conductances && IS_REGULAR_NEURON(lNId, networkConfigs[netId].numNReg, networkConfigs[netId].numNPois)) {
						cpuRuntimeData[netId].gAMPA[lNId]  *= dAMPA;
						if (sim_with_NMDA_rise) {
							cpuRuntimeData[netId].gNMDA_r[lNId] *= rNMDA;	// rise
							cpuRuntimeData[netId].gNMDA_d[lNId] *= dNMDA;	// decay
						} else {
							cpuRuntimeData[netId].gNMDA[lNId]   *= dNMDA;	// instantaneous rise
						}
						
						cpuRuntimeData[netId].gGABAa[lNId] *= dGABAa;
						if (sim_with_GABAb_rise) {
							cpuRuntimeData[netId].gGABAb_r[lNId] *= rGABAb;	// rise
							cpuRuntimeData[netId].gGABAb_d[lNId] *= dGABAb;	// decay
						} else {
							cpuRuntimeData[netId].gGABAb[lNId] *= dGABAb;	// instantaneous rise
						}
					}
				}
			}
		}
	}
}

// FIXME: wrong to use groupConfigs[0]
void SNN::findFiring() {
	// ToDo: This can be further optimized using multiple threads allocated on mulitple CPU cores
	for (int netId = 0; netId < MAX_NET_PER_SNN; netId++) {
		if (!groupPartitionLists[netId].empty()) {
			for(int lGrpId = 0; lGrpId < networkConfigs[netId].numGroups; lGrpId++) {
				for (int lNId = 0; lNId < groupConfigs[netId][lGrpId].numN; lNId++) {
					bool needToWrite = false;
					// given group of neurons belong to the poisson group....
					if (groupConfigs[netId][lGrpId].Type & POISSON_NEURON) {
						if(groupConfigs[netId][lGrpId].isSpikeGenFunc) {
							unsigned int offset = lNId - groupConfigs[netId][lGrpId].lStartN + groupConfigs[netId][lGrpId].Noffset;
							needToWrite = getSpikeGenBit(offset, netId);
						} else { // spikes generated by poission rate
							needToWrite = getPoissonSpike(lNId, netId);
						}
					} else if (cpuRuntimeData[netId].voltage[lNId] >= 30.0f) {
						needToWrite = true;
					}

					// his flag is set if with_stdp is set and also grpType is set to have GROUP_SYN_FIXED
					if (needToWrite) {
						// update spike count: spikeCountD2Sec(W), spikeCountD1Sec(W), spikeCountLastSecLeftD2(R)

						// update firing table: firingTableD1(W), firingTableD2(W)

						// update external firing table: extFiringTableEndIdxD1(W), extFiringTableEndIdxD2(W), extFiringTableD1(W), extFiringTableD2(W)

						// update STP for neurons that fire
						if (groupConfigs[netId][lGrpId].WithSTP) {
							firingUpdateSTP(lNId, lGrpId, netId);
						}
												
						// keep track of number spikes per neuron
						cpuRuntimeData[netId].nSpikeCnt[lNId]++;

						if (IS_REGULAR_NEURON(lNId, networkConfigs[netId].numNReg, networkConfigs[netId].numNPois))
							resetFiredNeuron(lNId, lGrpId, netId);

						// STDP calculation: the post-synaptic neuron fires after the arrival of a pre-synaptic spike
						if (!sim_in_testing && groupConfigs[netId][lGrpId].WithSTDP) {
							updateLTP(lNId, lGrpId, netId);
						}
					}
				}
			}
		}
	}
}

void SNN::updateLTP(int lNId, int lGrpId, int netId) {
	unsigned int pos_ij = cpuRuntimeData[netId].cumulativePre[lNId]; // the index of pre-synaptic neuron
	for(int j = 0; j < managerRuntimeData.Npre_plastic[lNId]; pos_ij++, j++) {
		int stdp_tDiff = (simTime - cpuRuntimeData[netId].synSpikeTime[pos_ij]);
		assert(!((stdp_tDiff < 0) && (cpuRuntimeData[netId].synSpikeTime[pos_ij] != MAX_SIMULATION_TIME)));

		if (stdp_tDiff > 0) {
			// check this is an excitatory or inhibitory synapse
			if (groupConfigs[netId][lGrpId].WithESTDP && cpuRuntimeData[netId].maxSynWt[pos_ij] >= 0) { // excitatory synapse
				// Handle E-STDP curve
				switch (groupConfigs[netId][lGrpId].WithESTDPcurve) {
				case EXP_CURVE: // exponential curve
					if (stdp_tDiff * groupConfigs[netId][lGrpId].TAU_PLUS_INV_EXC < 25)
						cpuRuntimeData[netId].wtChange[pos_ij] += STDP(stdp_tDiff, groupConfigs[netId][lGrpId].ALPHA_PLUS_EXC, groupConfigs[netId][lGrpId].TAU_PLUS_INV_EXC);
					break;
				case TIMING_BASED_CURVE: // sc curve
					if (stdp_tDiff * groupConfigs[netId][lGrpId].TAU_PLUS_INV_EXC < 25) {
						if (stdp_tDiff <= groupConfigs[netId][lGrpId].GAMMA)
							cpuRuntimeData[netId].wtChange[pos_ij] += groupConfigs[netId][lGrpId].OMEGA + groupConfigs[netId][lGrpId].KAPPA * STDP(stdp_tDiff, groupConfigs[netId][lGrpId].ALPHA_PLUS_EXC, groupConfigs[netId][lGrpId].TAU_PLUS_INV_EXC);
						else // stdp_tDiff > GAMMA
							cpuRuntimeData[netId].wtChange[pos_ij] -= STDP(stdp_tDiff, groupConfigs[netId][lGrpId].ALPHA_PLUS_EXC, groupConfigs[netId][lGrpId].TAU_PLUS_INV_EXC);
					}
					break;
				default:
					KERNEL_ERROR("Invalid E-STDP curve!");
					break;
				}
			} else if (groupConfigs[netId][lGrpId].WithISTDP && cpuRuntimeData[netId].maxSynWt[pos_ij] < 0) { // inhibitory synapse
				// Handle I-STDP curve
				switch (groupConfigs[netId][lGrpId].WithISTDPcurve) {
				case EXP_CURVE: // exponential curve
					if (stdp_tDiff * groupConfigs[netId][lGrpId].TAU_PLUS_INV_INB < 25) { // LTP of inhibitory synapse, which decreases synapse weight
						cpuRuntimeData[netId].wtChange[pos_ij] -= STDP(stdp_tDiff, groupConfigs[netId][lGrpId].ALPHA_PLUS_INB, groupConfigs[netId][lGrpId].TAU_PLUS_INV_INB);
					}
					break;
				case PULSE_CURVE: // pulse curve
					if (stdp_tDiff <= groupConfigs[netId][lGrpId].LAMBDA) { // LTP of inhibitory synapse, which decreases synapse weight
						cpuRuntimeData[netId].wtChange[pos_ij] -= groupConfigs[netId][lGrpId].BETA_LTP;
						//printf("I-STDP LTP\n");
					} else if (stdp_tDiff <= groupConfigs[netId][lGrpId].DELTA) { // LTD of inhibitory syanpse, which increase sysnapse weight
						cpuRuntimeData[netId].wtChange[pos_ij] -= groupConfigs[netId][lGrpId].BETA_LTD;
						//printf("I-STDP LTD\n");
					} else { /*do nothing*/}
					break;
				default:
					KERNEL_ERROR("Invalid I-STDP curve!");
					break;
				}
			}
		}
	}
}

void SNN::firingUpdateSTP(int lNId, int lGrpId, int netId) {
	// update the spike-dependent part of du/dt and dx/dt
	// we need to retrieve the STP values from the right buffer position (right before vs. right after the spike)
	int ind_plus = STP_BUF_POS(lNId, simTime, networkConfigs[netId].maxDelay); // index of right after the spike, such as in u^+
	int ind_minus = STP_BUF_POS(lNId, (simTime-1), networkConfigs[netId].maxDelay); // index of right before the spike, such as in u^-

	// du/dt = -u/tau_F + U * (1-u^-) * \delta(t-t_{spk})
	cpuRuntimeData[netId].stpu[ind_plus] += groupConfigs[netId][lGrpId].STP_U * (1.0f - cpuRuntimeData[netId].stpu[ind_minus]);

	// dx/dt = (1-x)/tau_D - u^+ * x^- * \delta(t-t_{spk})
	cpuRuntimeData[netId].stpx[ind_plus] -= cpuRuntimeData[netId].stpu[ind_plus] * cpuRuntimeData[netId].stpx[ind_minus];
}

void SNN::resetFiredNeuron(int lNId, short int lGrpId, int netId) {
	// \FIXME \TODO: convert this to use coalesced access by grouping into a
	// single 16 byte access. This might improve bandwidth performance
	// This is fully uncoalsced access...need to convert to coalsced access..
	cpuRuntimeData[netId].voltage[lNId] = cpuRuntimeData[netId].Izh_c[lNId];
	cpuRuntimeData[netId].recovery[lNId] += cpuRuntimeData[netId].Izh_d[lNId];
	if (groupConfigs[netId][lGrpId].WithSTDP)
		cpuRuntimeData[netId].lastSpikeTime[lNId] = simTime;
	
	if (networkConfigs[netId].sim_with_homeostasis) {
		// with homeostasis flag can be used here.
		cpuRuntimeData[netId].avgFiring[lNId] += 1000 / (groupConfigs[netId][lGrpId].avgTimeScale * 1000);
	}
}

bool SNN::getPoissonSpike(int lNId, int netId) {
	// Random number value is less than the poisson firing probability
	// if poisson firing probability is say 1.0 then the random poisson ptr
	// will always be less than 1.0 and hence it will continiously fire
	return cpuRuntimeData[netId].randNum[lNId - networkConfigs[netId].numNReg] * 1000.0f
			< cpuRuntimeData[netId].poissonFireRate[lNId - networkConfigs[netId].numNReg];
}

bool SNN::getSpikeGenBit(unsigned int nIdPos, int netId) {
	const int nIdBitPos = nIdPos % 32;
	const int nIdIndex  = nIdPos / 32;
	return ((cpuRuntimeData[netId].spikeGenBits[nIdIndex] >> nIdBitPos) & 0x1);
}

void SNN::generatePostSynapticSpike(unsigned int pre_i, unsigned int idx_d, unsigned int offset, int tD) {
	// get synaptic info...
	SynInfo post_info = managerRuntimeData.postSynapticIds[offset + idx_d];

	// get post-neuron id
	unsigned int post_i = GET_CONN_NEURON_ID(post_info);
	assert(post_i < glbNetworkConfig.numN);

	// get syn id
	int s_i = GET_CONN_SYN_ID(post_info);
	assert(s_i<(managerRuntimeData.Npre[post_i]));

	// get the cumulative position for quick access
	unsigned int pos_i = managerRuntimeData.cumulativePre[post_i] + s_i;
	assert(post_i < glbNetworkConfig.numNReg); // \FIXME is this assert supposed to be for pos_i?

	// get group id of pre- / post-neuron
	short int post_grpId = managerRuntimeData.grpIds[post_i];
	short int pre_grpId = managerRuntimeData.grpIds[pre_i];

	unsigned int pre_type = groupConfigs[0][pre_grpId].Type;

	// get connect info from the cumulative synapse index for mulSynFast/mulSynSlow (requires less memory than storing
	// mulSynFast/Slow per synapse or storing a pointer to grpConnectInfo_s)
	// mulSynFast will be applied to fast currents (either AMPA or GABAa)
	// mulSynSlow will be applied to slow currents (either NMDA or GABAb)
	short int mulIndex = managerRuntimeData.connIdsPreIdx[pos_i];
	assert(mulIndex>=0 && mulIndex<numConnections);


	// for each presynaptic spike, postsynaptic (synaptic) current is going to increase by some amplitude (change)
	// generally speaking, this amplitude is the weight; but it can be modulated by STP
	float change = managerRuntimeData.wt[pos_i];

	if (groupConfigs[0][pre_grpId].WithSTP) {
		// if pre-group has STP enabled, we need to modulate the weight
		// NOTE: Order is important! (Tsodyks & Markram, 1998; Mongillo, Barak, & Tsodyks, 2008)
		// use u^+ (value right after spike-update) but x^- (value right before spike-update)

		// dI/dt = -I/tau_S + A * u^+ * x^- * \delta(t-t_{spk})
		// I noticed that for connect(.., RangeDelay(1), ..) tD will be 0
		int ind_minus = STP_BUF_POS(pre_i, (simTime-tD-1), glbNetworkConfig.maxDelay);
		int ind_plus  = STP_BUF_POS(pre_i, (simTime-tD), glbNetworkConfig.maxDelay);

		change *= groupConfigs[0][pre_grpId].STP_A*managerRuntimeData.stpu[ind_plus]*managerRuntimeData.stpx[ind_minus];

//		fprintf(stderr,"%d: %d[%d], numN=%d, td=%d, maxDelay_=%d, ind-=%d, ind+=%d, stpu=[%f,%f], stpx=[%f,%f], change=%f, wt=%f\n",
//			simTime, pre_grpId, pre_i,
//					numN, tD, maxDelay_, ind_minus, ind_plus,
//					stpu[ind_minus], stpu[ind_plus], stpx[ind_minus], stpx[ind_plus], change, wt[pos_i]);
	}

	// update currents
	// NOTE: it's faster to += 0.0 rather than checking for zero and not updating
	if (sim_with_conductances) {
		if (pre_type & TARGET_AMPA) // if post_i expresses AMPAR
			managerRuntimeData.gAMPA [post_i] += change*mulSynFast[mulIndex]; // scale by some factor
		if (pre_type & TARGET_NMDA) {
			if (sim_with_NMDA_rise) {
				managerRuntimeData.gNMDA_r[post_i] += change*sNMDA*mulSynSlow[mulIndex];
				managerRuntimeData.gNMDA_d[post_i] += change*sNMDA*mulSynSlow[mulIndex];
			} else {
				managerRuntimeData.gNMDA [post_i] += change*mulSynSlow[mulIndex];
			}
		}
		if (pre_type & TARGET_GABAa)
			managerRuntimeData.gGABAa[post_i] -= change*mulSynFast[mulIndex]; // wt should be negative for GABAa and GABAb
		if (pre_type & TARGET_GABAb) {
			if (sim_with_GABAb_rise) {
				managerRuntimeData.gGABAb_r[post_i] -= change*sGABAb*mulSynSlow[mulIndex];
				managerRuntimeData.gGABAb_d[post_i] -= change*sGABAb*mulSynSlow[mulIndex];
			} else {
				managerRuntimeData.gGABAb[post_i] -= change*mulSynSlow[mulIndex];
			}
		}
	} else {
		managerRuntimeData.current[post_i] += change;
	}

	managerRuntimeData.synSpikeTime[pos_i] = simTime;

	// Got one spike from dopaminergic neuron, increase dopamine concentration in the target area
	if (pre_type & TARGET_DA) {
		managerRuntimeData.grpDA[post_grpId] += 0.04;
	}

	// STDP calculation: the post-synaptic neuron fires before the arrival of a pre-synaptic spike
	if (!sim_in_testing && groupConfigs[0][post_grpId].WithSTDP) {
		int stdp_tDiff = (simTime-managerRuntimeData.lastSpikeTime[post_i]);

		if (stdp_tDiff >= 0) {
			if (groupConfigs[0][post_grpId].WithISTDP && ((pre_type & TARGET_GABAa) || (pre_type & TARGET_GABAb))) { // inhibitory syanpse
				// Handle I-STDP curve
				switch (groupConfigs[0][post_grpId].WithISTDPcurve) {
				case EXP_CURVE: // exponential curve
					if ((stdp_tDiff*groupConfigs[0][post_grpId].TAU_MINUS_INV_INB)<25) { // LTD of inhibitory syanpse, which increase synapse weight
						managerRuntimeData.wtChange[pos_i] -= STDP(stdp_tDiff, groupConfigs[0][post_grpId].ALPHA_MINUS_INB, groupConfigs[0][post_grpId].TAU_MINUS_INV_INB);
					}
					break;
				case PULSE_CURVE: // pulse curve
					if (stdp_tDiff <= groupConfigs[0][post_grpId].LAMBDA) { // LTP of inhibitory synapse, which decreases synapse weight
						managerRuntimeData.wtChange[pos_i] -= groupConfigs[0][post_grpId].BETA_LTP;
					} else if (stdp_tDiff <= groupConfigs[0][post_grpId].DELTA) { // LTD of inhibitory syanpse, which increase synapse weight
						managerRuntimeData.wtChange[pos_i] -= groupConfigs[0][post_grpId].BETA_LTD;
					} else { /*do nothing*/ }
					break;
				default:
					KERNEL_ERROR("Invalid I-STDP curve");
					break;
				}
			} else if (groupConfigs[0][post_grpId].WithESTDP && ((pre_type & TARGET_AMPA) || (pre_type & TARGET_NMDA))) { // excitatory synapse
				// Handle E-STDP curve
				switch (groupConfigs[0][post_grpId].WithESTDPcurve) {
				case EXP_CURVE: // exponential curve
				case TIMING_BASED_CURVE: // sc curve
					if (stdp_tDiff * groupConfigs[0][post_grpId].TAU_MINUS_INV_EXC < 25)
						managerRuntimeData.wtChange[pos_i] += STDP(stdp_tDiff, groupConfigs[0][post_grpId].ALPHA_MINUS_EXC, groupConfigs[0][post_grpId].TAU_MINUS_INV_EXC);
					break;
				default:
					KERNEL_ERROR("Invalid E-STDP curve");
					break;
				}
			} else { /*do nothing*/ }
		}
		assert(!((stdp_tDiff < 0) && (managerRuntimeData.lastSpikeTime[post_i] != MAX_SIMULATION_TIME)));
	}
}

// FIXME: wrong to use groupConfigs[0]
void  SNN::globalStateUpdate() {
	double tmp_iNMDA, tmp_I;
	double tmp_gNMDA, tmp_gGABAb;

	for(int g = 0; g < numGroups; g++) {
		if (groupConfigs[0][g].Type & POISSON_NEURON) {
			if (groupConfigs[0][g].WithHomeostasis) {
				for(int i=groupConfigs[0][g].gStartN; i <= groupConfigs[0][g].gEndN; i++)
					managerRuntimeData.avgFiring[i] *= groupConfigs[0][g].avgTimeScale_decay;
			}
			continue;
		}

		// decay dopamine concentration
		if ((groupConfigs[0][g].WithESTDPtype == DA_MOD || groupConfigs[0][g].WithISTDP == DA_MOD) && managerRuntimeData.grpDA[g] > groupConfigs[0][g].baseDP) {
			managerRuntimeData.grpDA[g] *= groupConfigs[0][g].decayDP;
		}
		managerRuntimeData.grpDABuffer[g * 1000 + simTimeMs] = managerRuntimeData.grpDA[g];

		for(int i=groupConfigs[0][g].gStartN; i <= groupConfigs[0][g].gEndN; i++) {
			assert(i < glbNetworkConfig.numNReg);
			// update average firing rate for homeostasis
			if (groupConfigs[0][g].WithHomeostasis)
				managerRuntimeData.avgFiring[i] *= groupConfigs[0][g].avgTimeScale_decay;

			// update conductances
			if (sim_with_conductances) {
				// COBA model

				// all the tmpIs will be summed into current[i] in the following loop
				managerRuntimeData.current[i] = 0.0f;

				// \FIXME: these tmp vars cause a lot of rounding errors... consider rewriting
				for (int j=0; j<COND_INTEGRATION_SCALE; j++) {
					tmp_iNMDA = (managerRuntimeData.voltage[i]+80.0)*(managerRuntimeData.voltage[i]+80.0)/60.0/60.0;

					tmp_gNMDA = sim_with_NMDA_rise ? managerRuntimeData.gNMDA_d[i]-managerRuntimeData.gNMDA_r[i] : managerRuntimeData.gNMDA[i];
					tmp_gGABAb = sim_with_GABAb_rise ? managerRuntimeData.gGABAb_d[i]-managerRuntimeData.gGABAb_r[i] : managerRuntimeData.gGABAb[i];

					tmp_I = -(   managerRuntimeData.gAMPA[i]*(managerRuntimeData.voltage[i]-0)
									 + tmp_gNMDA*tmp_iNMDA/(1+tmp_iNMDA)*(managerRuntimeData.voltage[i]-0)
									 + managerRuntimeData.gGABAa[i]*(managerRuntimeData.voltage[i]+70)
									 + tmp_gGABAb*(managerRuntimeData.voltage[i]+90)
								   );

					managerRuntimeData.voltage[i] += ((0.04*managerRuntimeData.voltage[i]+5.0)*managerRuntimeData.voltage[i]+140.0-managerRuntimeData.recovery[i]+tmp_I+managerRuntimeData.extCurrent[i])
						/ COND_INTEGRATION_SCALE;
					assert(!isnan(managerRuntimeData.voltage[i]) && !isinf(managerRuntimeData.voltage[i]));

					// keep track of total current
					managerRuntimeData.current[i] += tmp_I;

					if (managerRuntimeData.voltage[i] > 30) {
						managerRuntimeData.voltage[i] = 30;
						j=COND_INTEGRATION_SCALE; // break the loop but evaluate u[i]
//						if (gNMDA[i]>=10.0f) KERNEL_WARN("High NMDA conductance (gNMDA>=10.0) may cause instability");
//						if (gGABAb[i]>=2.0f) KERNEL_WARN("High GABAb conductance (gGABAb>=2.0) may cause instability");
					}
					if (managerRuntimeData.voltage[i] < -90)
						managerRuntimeData.voltage[i] = -90;
					managerRuntimeData.recovery[i]+=managerRuntimeData.Izh_a[i]*(managerRuntimeData.Izh_b[i]*managerRuntimeData.voltage[i]-managerRuntimeData.recovery[i])/COND_INTEGRATION_SCALE;
				} // end COND_INTEGRATION_SCALE loop
			} else {
				// CUBA model
				managerRuntimeData.voltage[i] += 0.5*((0.04*managerRuntimeData.voltage[i]+5.0)*managerRuntimeData.voltage[i] + 140.0 - managerRuntimeData.recovery[i]
					+ managerRuntimeData.current[i] + managerRuntimeData.extCurrent[i]); //for numerical stability
				managerRuntimeData.voltage[i] += 0.5*((0.04*managerRuntimeData.voltage[i]+5.0)*managerRuntimeData.voltage[i] + 140.0 - managerRuntimeData.recovery[i]
					+ managerRuntimeData.current[i] + managerRuntimeData.extCurrent[i]); //time step is 0.5 ms
				if (managerRuntimeData.voltage[i] > 30)
					managerRuntimeData.voltage[i] = 30;
				if (managerRuntimeData.voltage[i] < -90)
					managerRuntimeData.voltage[i] = -90;
				managerRuntimeData.recovery[i]+=managerRuntimeData.Izh_a[i]*(managerRuntimeData.Izh_b[i]*managerRuntimeData.voltage[i]-managerRuntimeData.recovery[i]);
			} // end COBA/CUBA
		} // end StartN...EndN
	} // end numGroups
}

// FIXME: wrong to use groupConfigs[0]
// This function updates the synaptic weights from its derivatives..
void SNN::updateWeights() {
	// at this point we have already checked for sim_in_testing and sim_with_fixedwts
	assert(sim_in_testing==false);
	assert(sim_with_fixedwts==false);

	// update synaptic weights here for all the neurons..
	for(int g = 0; g < numGroups; g++) {
		// no changable weights so continue without changing..
		if(groupConfigs[0][g].FixedInputWts || !(groupConfigs[0][g].WithSTDP))
			continue;

		for(int i = groupConfigs[0][g].gStartN; i <= groupConfigs[0][g].gEndN; i++) {
			assert(i < glbNetworkConfig.numNReg);
			unsigned int offset = managerRuntimeData.cumulativePre[i];
			float diff_firing = 0.0;
			float homeostasisScale = 1.0;

			if(groupConfigs[0][g].WithHomeostasis) {
				assert(managerRuntimeData.baseFiring[i]>0);
				diff_firing = 1-managerRuntimeData.avgFiring[i]/managerRuntimeData.baseFiring[i];
				homeostasisScale = groupConfigs[0][g].homeostasisScale;
			}

			if (i==groupConfigs[0][g].gStartN)
				KERNEL_DEBUG("Weights, Change at %d (diff_firing: %f)", simTimeSec, diff_firing);

			for(int j = 0; j < managerRuntimeData.Npre_plastic[i]; j++) {
				//	if (i==groupConfigs[0][g].StartN)
				//		KERNEL_DEBUG("%1.2f %1.2f \t", wt[offset+j]*10, wtChange[offset+j]*10);
				float effectiveWtChange = stdpScaleFactor_ * managerRuntimeData.wtChange[offset + j];
//				if (wtChange[offset+j])
//					printf("connId=%d, wtChange[%d]=%f\n",connIdsPreIdx[offset+j],offset+j,wtChange[offset+j]);

				// homeostatic weight update
				// FIXME: check WithESTDPtype and WithISTDPtype first and then do weight change update
				switch (groupConfigs[0][g].WithESTDPtype) {
				case STANDARD:
					if (groupConfigs[0][g].WithHomeostasis) {
						managerRuntimeData.wt[offset+j] += (diff_firing*managerRuntimeData.wt[offset+j]*homeostasisScale + managerRuntimeData.wtChange[offset+j])*managerRuntimeData.baseFiring[i]/groupConfigs[0][g].avgTimeScale/(1+fabs(diff_firing)*50);
					} else {
						// just STDP weight update
						managerRuntimeData.wt[offset+j] += effectiveWtChange;
					}
					break;
				case DA_MOD:
					if (groupConfigs[0][g].WithHomeostasis) {
						effectiveWtChange = managerRuntimeData.grpDA[g] * effectiveWtChange;
						managerRuntimeData.wt[offset+j] += (diff_firing*managerRuntimeData.wt[offset+j]*homeostasisScale + effectiveWtChange)*managerRuntimeData.baseFiring[i]/groupConfigs[0][g].avgTimeScale/(1+fabs(diff_firing)*50);
					} else {
						managerRuntimeData.wt[offset+j] += managerRuntimeData.grpDA[g] * effectiveWtChange;
					}
					break;
				case UNKNOWN_STDP:
				default:
					// we shouldn't even be in here if !WithSTDP
					break;
				}

				switch (groupConfigs[0][g].WithISTDPtype) {
				case STANDARD:
					if (groupConfigs[0][g].WithHomeostasis) {
						managerRuntimeData.wt[offset+j] += (diff_firing*managerRuntimeData.wt[offset+j]*homeostasisScale + managerRuntimeData.wtChange[offset+j])*managerRuntimeData.baseFiring[i]/groupConfigs[0][g].avgTimeScale/(1+fabs(diff_firing)*50);
					} else {
						// just STDP weight update
						managerRuntimeData.wt[offset+j] += effectiveWtChange;
					}
					break;
				case DA_MOD:
					if (groupConfigs[0][g].WithHomeostasis) {
						effectiveWtChange = managerRuntimeData.grpDA[g] * effectiveWtChange;
						managerRuntimeData.wt[offset+j] += (diff_firing*managerRuntimeData.wt[offset+j]*homeostasisScale + effectiveWtChange)*managerRuntimeData.baseFiring[i]/groupConfigs[0][g].avgTimeScale/(1+fabs(diff_firing)*50);
					} else {
						managerRuntimeData.wt[offset+j] += managerRuntimeData.grpDA[g] * effectiveWtChange;
					}
					break;
				case UNKNOWN_STDP:
				default:
					// we shouldn't even be in here if !WithSTDP
					break;
				}

				// It is users' choice to decay weight change or not
				// see setWeightAndWeightChangeUpdate()
				managerRuntimeData.wtChange[offset+j] *= wtChangeDecay_;

				// if this is an excitatory or inhibitory synapse
				if (managerRuntimeData.maxSynWt[offset + j] >= 0) {
					if (managerRuntimeData.wt[offset + j] >= managerRuntimeData.maxSynWt[offset + j])
						managerRuntimeData.wt[offset + j] = managerRuntimeData.maxSynWt[offset + j];
					if (managerRuntimeData.wt[offset + j] < 0)
						managerRuntimeData.wt[offset + j] = 0.0;
				} else {
					if (managerRuntimeData.wt[offset + j] <= managerRuntimeData.maxSynWt[offset + j])
						managerRuntimeData.wt[offset + j] = managerRuntimeData.maxSynWt[offset + j];
					if (managerRuntimeData.wt[offset+j] > 0)
						managerRuntimeData.wt[offset+j] = 0.0;
				}
			}
		}
	}
}

/*!
 * \brief This function is called every second by SNN::runNetwork(). It updates the firingTableD1(D2) and
 * timeTableD1(D2) by removing older firing information.
 */
void SNN::shiftSpikeTables() {
	// Read the neuron ids that fired in the last glbNetworkConfig.maxDelay seconds
	// and put it to the beginning of the firing table...
	for (int netId = 0; netId < MAX_NET_PER_SNN; netId++) {
		if (!groupPartitionLists[netId].empty()) {
			for(int p = cpuRuntimeData[netId].timeTableD2[999], k = 0; p < cpuRuntimeData[netId].timeTableD2[999 + glbNetworkConfig.maxDelay + 1]; p++, k++) {
				cpuRuntimeData[netId].firingTableD2[k] = cpuRuntimeData[netId].firingTableD2[p];
			}

			for(int i = 0; i < glbNetworkConfig.maxDelay; i++) {
				cpuRuntimeData[netId].timeTableD2[i + 1] = cpuRuntimeData[netId].timeTableD2[1000 + i + 1] - cpuRuntimeData[netId].timeTableD2[1000];
			}

			cpuRuntimeData[netId].timeTableD1[glbNetworkConfig.maxDelay] = 0;

			/* the code of weight update has been moved to SNN::updateWeights() */

			cpuRuntimeData[netId].spikeCount	+= cpuRuntimeData[netId].spikeCountSec;
			cpuRuntimeData[netId].spikeCountD2 += (cpuRuntimeData[netId].spikeCountD2Sec - cpuRuntimeData[netId].timeTableD2[glbNetworkConfig.maxDelay]);
			cpuRuntimeData[netId].spikeCountD1 += cpuRuntimeData[netId].spikeCountD1Sec;

			cpuRuntimeData[netId].spikeCountD1Sec  = 0;
			cpuRuntimeData[netId].spikeCountSec = 0;
			cpuRuntimeData[netId].spikeCountD2Sec = cpuRuntimeData[netId].timeTableD2[glbNetworkConfig.maxDelay];
		}
	}
}

void SNN::allocateSNN_CPU(int netId) {
	// setup memory type of CPU runtime data
	cpuRuntimeData[netId].memType = CPU_MODE;

	// display some memory management info
	//size_t avail, total, previous;
	//float toMB = std::pow(1024.0f, 2);
	//KERNEL_INFO("CPU Memory Management: (Total %2.3f MB)",(float)(total/toMB));
	//KERNEL_INFO("Data\t\t\tSize\t\tTotal Used\tTotal Available");
	//KERNEL_INFO("Init:\t\t\t%2.3f MB\t%2.3f MB\t%2.3f MB",(float)(total)/toMB,(float)((total-avail)/toMB), (float)(avail/toMB));
	//previous=avail;

	//// FIXME: necessary for CPU_MODE?? allocate random number generator on CPU(s)
	//if(gpuRuntimeData[netId].gpuRandGen == NULL) {
	//	curandCreateGenerator(&gpuRuntimeData[netId].gpuRandGen, CURAND_RNG_PSEUDO_DEFAULT);
	//	curandSetPseudoRandomGeneratorSeed(gpuRuntimeData[netId].gpuRandGen, randSeed_ + netId);
	//}

	//// allocate SNN::gpuRuntimeData[0].randNum for random number generators
	//CUDA_CHECK_ERRORS(cudaMalloc((void **)&gpuRuntimeData[netId].randNum, networkConfigs[netId].numNPois * sizeof(float)));
	//KERNEL_INFO("Random Gen:\t\t%2.3f MB\t%2.3f MB\t%2.3f MB",(float)(previous-avail)/toMB, (float)((total-avail)/toMB),(float)(avail/toMB));
	//previous=avail;


	// initialize (copy from SNN) cpuRuntimeData[0].Npre, cpuRuntimeData[0].Npre_plastic, cpuRuntimeData[0].Npre_plasticInv, cpuRuntimeData[0].cumulativePre
	// initialize (copy from SNN) cpuRuntimeData[0].cumulativePost, cpuRuntimeData[0].Npost, cpuRuntimeData[0].postDelayInfo
	// initialize (copy from SNN) cpuRuntimeData[0].postSynapticIds, cpuRuntimeData[0].preSynapticIds
	copyPreConnectionInfo(netId, ALL, &cpuRuntimeData[netId], &managerRuntimeData, true);
	copyPostConnectionInfo(netId, ALL, &cpuRuntimeData[netId], &managerRuntimeData, true);
	//KERNEL_INFO("Conn Info:\t\t%2.3f MB\t%2.3f MB\t%2.3f MB",(float)(previous-avail)/toMB,(float)((total-avail)/toMB), (float)(avail/toMB));
	//previous=avail;
	
	// initialize (copy from SNN) cpuRuntimeData[0].wt, cpuRuntimeData[0].wtChange, cpuRuntimeData[0].maxSynWt
	copySynapseState(netId, &cpuRuntimeData[netId], true);
	//KERNEL_INFO("Syn State:\t\t%2.3f MB\t%2.3f MB\t%2.3f MB",(float)(previous-avail)/toMB,(float)((total-avail)/toMB), (float)(avail/toMB));
	//previous=avail;
	
	// copy the neuron state information to the GPU..
	// initialize (copy from managerRuntimeData) cpuRuntimeData[0].recovery, cpuRuntimeData[0].voltage, cpuRuntimeData[0].current
	// initialize (copy from managerRuntimeData) cpuRuntimeData[0].gGABAa, cpuRuntimeData[0].gGABAb, cpuRuntimeData[0].gAMPA, cpuRuntimeData[0].gNMDA
	// initialize (copy from SNN) cpuRuntimeData[0].Izh_a, cpuRuntimeData[0].Izh_b, cpuRuntimeData[0].Izh_c, cpuRuntimeData[0].Izh_d
	// initialize (copy form SNN) cpuRuntimeData[0].baseFiring, cpuRuntimeData[0].baseFiringInv
	copyNeuronState(netId, ALL, &cpuRuntimeData[netId], true);

	// copy STP state, considered as neuron state
	if (sim_with_stp) {
		// initialize (copy from SNN) stpu, stpx
		copySTPState(netId, ALL, &cpuRuntimeData[netId], &managerRuntimeData, true);
	}
	//KERNEL_INFO("Neuron State:\t\t%2.3f MB\t%2.3f MB\t%2.3f MB",(float)(previous-avail)/toMB,(float)((total-avail)/toMB), (float)(avail/toMB));
	//previous=avail;
		
	// initialize (copy from SNN) cpuRuntimeData[0].grpDA(5HT,ACh,NE)
	// initialize (copy from SNN) cpuRuntimeData[0].grpDA(5HT,ACh,NE)Buffer[]
	copyGroupState(netId, ALL, &cpuRuntimeData[netId], &managerRuntimeData, true);
	//KERNEL_INFO("Group State:\t\t%2.3f MB\t%2.3f MB\t%2.3f MB",(float)(previous-avail)/toMB,(float)((total-avail)/toMB), (float)(avail/toMB));
	//previous=avail;

	// initialize (cudaMemset) cpuRuntimeData[0].I_set, cpuRuntimeData[0].poissonFireRate
	// initialize (copy from SNN) cpuRuntimeData[0].firingTableD1, cpuRuntimeData[0].firingTableD2
	// initialize (cudaMalloc) cpuRuntimeData[0].spikeGenBits
	// initialize (copy from managerRuntimeData) cpuRuntimeData[0].nSpikeCnt,
	// initialize (copy from SNN) cpuRuntimeData[0].synSpikeTime, cpuRuntimeData[0].lastSpikeTime
	copyAuxiliaryData(netId, ALL, &cpuRuntimeData[netId], true);
	//KERNEL_INFO("Auxiliary Data:\t\t%2.3f MB\t%2.3f MB\t%2.3f MB\n\n",(float)(previous-avail)/toMB,(float)((total-avail)/toMB), (float)(avail/toMB));
	//previous=avail;

	// TODO: move mulSynFast, mulSynSlow to ConnectConfig structure
	// copy connection configs
	//CUDA_CHECK_ERRORS(cudaMemcpyToSymbol(d_mulSynFast, mulSynFast, sizeof(float) * networkConfigs[netId].numConnections, 0, cudaMemcpyHostToDevice));
	//CUDA_CHECK_ERRORS(cudaMemcpyToSymbol(d_mulSynSlow, mulSynSlow, sizeof(float) * networkConfigs[netId].numConnections, 0, cudaMemcpyHostToDevice));

	KERNEL_DEBUG("Transfering group settings to CPU:");
	for (int lGrpId = 0; lGrpId < networkConfigs[netId].numAssignedGroups; lGrpId++) {
		KERNEL_DEBUG("Settings for Group %s:", groupConfigMap[groupConfigs[netId][lGrpId].gGrpId].grpName.c_str());
		
		KERNEL_DEBUG("\tType: %d",(int)groupConfigs[netId][lGrpId].Type);
		KERNEL_DEBUG("\tNumN: %d",groupConfigs[netId][lGrpId].numN);
		KERNEL_DEBUG("\tM: %d",groupConfigs[netId][lGrpId].numPostSynapses);
		KERNEL_DEBUG("\tPreM: %d",groupConfigs[netId][lGrpId].numPreSynapses);
		KERNEL_DEBUG("\tspikeGenerator: %d",(int)groupConfigs[netId][lGrpId].isSpikeGenerator);
		KERNEL_DEBUG("\tFixedInputWts: %d",(int)groupConfigs[netId][lGrpId].FixedInputWts);
		KERNEL_DEBUG("\tMaxDelay: %d",(int)groupConfigs[netId][lGrpId].MaxDelay);
		KERNEL_DEBUG("\tWithSTDP: %d",(int)groupConfigs[netId][lGrpId].WithSTDP);
		if (groupConfigs[netId][lGrpId].WithSTDP) {
			KERNEL_DEBUG("\t\tE-STDP type: %s",stdpType_string[groupConfigs[netId][lGrpId].WithESTDPtype]);
			KERNEL_DEBUG("\t\tTAU_PLUS_INV_EXC: %f",groupConfigs[netId][lGrpId].TAU_PLUS_INV_EXC);
			KERNEL_DEBUG("\t\tTAU_MINUS_INV_EXC: %f",groupConfigs[netId][lGrpId].TAU_MINUS_INV_EXC);
			KERNEL_DEBUG("\t\tALPHA_PLUS_EXC: %f",groupConfigs[netId][lGrpId].ALPHA_PLUS_EXC);
			KERNEL_DEBUG("\t\tALPHA_MINUS_EXC: %f",groupConfigs[netId][lGrpId].ALPHA_MINUS_EXC);
			KERNEL_DEBUG("\t\tI-STDP type: %s",stdpType_string[groupConfigs[netId][lGrpId].WithISTDPtype]);
			KERNEL_DEBUG("\t\tTAU_PLUS_INV_INB: %f",groupConfigs[netId][lGrpId].TAU_PLUS_INV_INB);
			KERNEL_DEBUG("\t\tTAU_MINUS_INV_INB: %f",groupConfigs[netId][lGrpId].TAU_MINUS_INV_INB);
			KERNEL_DEBUG("\t\tALPHA_PLUS_INB: %f",groupConfigs[netId][lGrpId].ALPHA_PLUS_INB);
			KERNEL_DEBUG("\t\tALPHA_MINUS_INB: %f",groupConfigs[netId][lGrpId].ALPHA_MINUS_INB);
			KERNEL_DEBUG("\t\tLAMBDA: %f",groupConfigs[netId][lGrpId].LAMBDA);
			KERNEL_DEBUG("\t\tDELTA: %f",groupConfigs[netId][lGrpId].DELTA);
			KERNEL_DEBUG("\t\tBETA_LTP: %f",groupConfigs[netId][lGrpId].BETA_LTP);
			KERNEL_DEBUG("\t\tBETA_LTD: %f",groupConfigs[netId][lGrpId].BETA_LTD);
		}
		KERNEL_DEBUG("\tWithSTP: %d",(int)groupConfigs[netId][lGrpId].WithSTP);
		if (groupConfigs[netId][lGrpId].WithSTP) {
			KERNEL_DEBUG("\t\tSTP_U: %f",groupConfigs[netId][lGrpId].STP_U);
//				KERNEL_DEBUG("\t\tSTP_tD: %f",groupConfigs[netId][lGrpId].STP_tD);
//				KERNEL_DEBUG("\t\tSTP_tF: %f",groupConfigs[netId][lGrpId].STP_tF);
		}
		KERNEL_DEBUG("\tspikeGen: %s", groupConfigs[netId][lGrpId].isSpikeGenFunc? "is Set" : "is not set ");
	}

	// allocation of CPU runtime data is done
	cpuRuntimeData[netId].allocated = true;
}

/*!
 * \brief this function allocates memory sapce and copies information of pre-connections to it
 *
 * This function:
 * initialize Npre_plasticInv
 * (allocate and) copy Npre, Npre_plastic, Npre_plasticInv, cumulativePre, preSynapticIds
 * (allocate and) copy Npost, cumulativePost, postSynapticIds, postDelayInfo
 *
 *
 * \param[in] netId the id of a local network, which is the same as the Core (CPU) id
 * \param[in] lGrpId the local group id in a local network, which specifiy the group(s) to be copied
 * \param[in] dest pointer to runtime data desitnation
 * \param[in] src pointer to runtime data source
 * \param[in] allocateMem a flag indicates whether allocating memory space before copying
 *
 * \sa allocateSNN_CPU
 * \since v4.0
 */
void SNN::copyPreConnectionInfo(int netId, int lGrpId, RuntimeData* dest, RuntimeData* src, bool allocateMem) {
	int lengthN, lengthSyn, posN, posSyn;

	if (lGrpId == ALL) {
		lengthN = networkConfigs[netId].numNAssigned;
		posN = 0;
	} else {
		lengthN = groupConfigs[netId][lGrpId].numN;
		posN = groupConfigs[netId][lGrpId].lStartN;
	}

	// connection synaptic lengths and cumulative lengths...
	if(allocateMem) 
		dest->Npre = new unsigned short[networkConfigs[netId].numNAssigned];
	memcpy(&dest->Npre[posN], &src->Npre[posN], sizeof(short) * lengthN);

	// we don't need these data structures if the network doesn't have any plastic synapses at all
	if (!sim_with_fixedwts) {
		// presyn excitatory connections
		if(allocateMem) 
			dest->Npre_plastic = new unsigned short[networkConfigs[netId].numNAssigned];
		memcpy(&dest->Npre_plastic[posN], &src->Npre_plastic[posN], sizeof(short) * lengthN);

		// Npre_plasticInv is only used on GPUs, only allocate and copy it during initialization
		if(allocateMem) {
			float* Npre_plasticInv = new float[networkConfigs[netId].numNAssigned];

			for (int i = 0; i < networkConfigs[netId].numNAssigned; i++)
				Npre_plasticInv[i] = 1.0f / managerRuntimeData.Npre_plastic[i];

			dest->Npre_plasticInv = new float[networkConfigs[netId].numNAssigned];
			memcpy(dest->Npre_plasticInv, Npre_plasticInv, sizeof(float) * networkConfigs[netId].numNAssigned);

			delete[] Npre_plasticInv;
		}
	}
		
	// beginning position for the pre-synaptic information
	if(allocateMem)
		dest->cumulativePre = new unsigned int[networkConfigs[netId].numNAssigned];
	memcpy(&dest->cumulativePre[posN], &src->cumulativePre[posN], sizeof(int) * lengthN);

	// Npre, cumulativePre has been copied to destination
	if (lGrpId == ALL) {
		lengthSyn = networkConfigs[netId].numPreSynNet;
		posSyn = 0;
	} else {
		lengthSyn = 0;
		for (int lNId = groupConfigs[netId][lGrpId].lStartN; lNId <= groupConfigs[netId][lGrpId].lEndN; lNId++)
			lengthSyn += dest->Npre[lNId];

		posSyn = dest->cumulativePre[groupConfigs[netId][lGrpId].lStartN];
	}

	if(allocateMem)
		dest->preSynapticIds = new SynInfo[networkConfigs[netId].numPreSynNet];
	memcpy(&dest->preSynapticIds[posSyn], &src->preSynapticIds[posSyn], sizeof(SynInfo) * lengthSyn);
}

/*!
 * \brief this function allocates memory sapce and copies information of post-connections to it
 *
 * This function:
 * (allocate and) copy Npost, cumulativePost, postSynapticIds, postDelayInfo
 *
 *
 * \param[in] netId the id of a local network, which is the same as the Core (CPU) id
 * \param[in] lGrpId the local group id in a local network, which specifiy the group(s) to be copied
 * \param[in] dest pointer to runtime data desitnation
 * \param[in] src pointer to runtime data source
 * \param[in] allocateMem a flag indicates whether allocating memory space before copying
 *
 * \sa allocateSNN_CPU
 * \since v4.0
 */
void SNN::copyPostConnectionInfo(int netId, int lGrpId, RuntimeData* dest, RuntimeData* src, bool allocateMem) {
	int lengthN, lengthSyn, posN, posSyn;

	if (lGrpId == ALL) {
		lengthN = networkConfigs[netId].numNAssigned;
		posN = 0;
	} else {
		lengthN = groupConfigs[netId][lGrpId].numN;
		posN = groupConfigs[netId][lGrpId].lStartN;
	}

	// number of postsynaptic connections
	if(allocateMem)
		dest->Npost = new unsigned short[networkConfigs[netId].numNAssigned];
	memcpy(&dest->Npost[posN], &src->Npost[posN], sizeof(short) * lengthN);
	
	// beginning position for the post-synaptic information
	if(allocateMem) 
		dest->cumulativePost = new unsigned int[networkConfigs[netId].numNAssigned];
	memcpy(&dest->cumulativePost[posN], &src->cumulativePost[posN], sizeof(int) * lengthN);

	
	// Npost, cumulativePost has been copied to destination
	if (lGrpId == ALL) {
		lengthSyn = networkConfigs[netId].numPostSynNet;
		posSyn = 0;
	} else {
		lengthSyn = 0;
		for (int lNId = groupConfigs[netId][lGrpId].lStartN; lNId <= groupConfigs[netId][lGrpId].lEndN; lNId++)
			lengthSyn += dest->Npost[lNId];

		posSyn = dest->cumulativePost[groupConfigs[netId][lGrpId].lStartN];
	}

	// actual post synaptic connection information...
	if(allocateMem)
		dest->postSynapticIds = new SynInfo[networkConfigs[netId].numPostSynNet];
	memcpy(&dest->postSynapticIds[posSyn], &src->postSynapticIds[posSyn], sizeof(SynInfo) * lengthSyn);

	// static specific mapping and actual post-synaptic delay metric
	if(allocateMem)
		dest->postDelayInfo = new DelayInfo[networkConfigs[netId].numNAssigned * (glbNetworkConfig.maxDelay + 1)];
	memcpy(&dest->postDelayInfo[posN * (glbNetworkConfig.maxDelay + 1)], &src->postDelayInfo[posN * (glbNetworkConfig.maxDelay + 1)], sizeof(DelayInfo) * lengthN * (glbNetworkConfig.maxDelay + 1));
}

/*!
 * \brief this function allocates memory sapce and copies variables related to syanpses to it
 *
 * This function:
 * (allocate and) copy wt, wtChange, maxSynWt
 *
 *
 * \param[in] netId the id of a local network, which is the same as the Core (CPU) id
 * \param[in] dest pointer to runtime data desitnation
 * \param[in] allocateMem a flag indicates whether allocating memory space before copying
 *
 * \sa allocateSNN_GPU
 * \since v4.0
 */
void SNN::copySynapseState(int netId, RuntimeData* dest, bool allocateMem) {
	assert(networkConfigs[netId].numPreSynNet > 0);

	// synaptic information based
	if(allocateMem)
		dest->wt = new float[networkConfigs[netId].numPreSynNet];
	memcpy(dest->wt, managerRuntimeData.wt, sizeof(float) * networkConfigs[netId].numPreSynNet);

	// we don't need these data structures if the network doesn't have any plastic synapses at all
	// they show up in gpuUpdateLTP() and updateSynapticWeights(), two functions that do not get called if
	// sim_with_fixedwts is set
	if (!sim_with_fixedwts) {
		// synaptic weight derivative
		if(allocateMem)
			dest->wtChange = new float[networkConfigs[netId].numPreSynNet];
		memcpy(dest->wtChange, managerRuntimeData.wtChange, sizeof(float) * networkConfigs[netId].numPreSynNet);

		// synaptic weight maximum value
		if(allocateMem)
			dest->maxSynWt = new float[networkConfigs[netId].numPreSynNet];
		memcpy(dest->maxSynWt, managerRuntimeData.maxSynWt, sizeof(float) * networkConfigs[netId].numPreSynNet);
	}
}

/*!
 * \brief this function allocates memory sapce and copies variables related to nueron state to it
 *
 * This function:
 * (allocate and) copy voltage, recovery, current, avgFiring 
 *
 * This funcion is called by allocateSNN_GPU(). Only copying from host to device is required
 *
 * \param[in] netId the id of a local network, which is the same as the Core (CPU) id
 * \param[in] lGrpId the local group id in a local network, which specifiy the group(s) to be copied
 * \param[in] dest pointer to runtime data desitnation
 * \param[in] allocateMem a flag indicates whether allocating memory space before copying
 *
 * \sa allocateSNN_CPU fetchNeuronState
 * \since v3.0
 */
void SNN::copyNeuronState(int netId, int lGrpId, RuntimeData* dest, bool allocateMem) {
	int ptrPos, length;

	if(lGrpId == ALL) {
		ptrPos  = 0;
		length  = networkConfigs[netId].numNReg;
	}
	else {
		ptrPos  = groupConfigs[netId][lGrpId].lStartN;
		length  = groupConfigs[netId][lGrpId].numN;
	}

	assert(length <= networkConfigs[netId].numNReg);
	assert(length >= 0);

	if(!allocateMem && groupConfigs[netId][lGrpId].Type & POISSON_NEURON)
		return;

	if(allocateMem)
		dest->recovery = new float[length];
	memcpy(&dest->recovery[ptrPos], &managerRuntimeData.recovery[ptrPos], sizeof(float) * length);

	if(allocateMem)
		dest->voltage = new float[length];
	memcpy(&dest->voltage[ptrPos], &managerRuntimeData.voltage[ptrPos], sizeof(float) * length);

	//neuron input current...
	if(allocateMem)
		dest->current = new float[length];
	memcpy(&dest->current[ptrPos], &managerRuntimeData.current[ptrPos], sizeof(float) * length);

	if (sim_with_conductances) {
	    //conductance information
		copyConductanceAMPA(netId, lGrpId, dest, &managerRuntimeData, allocateMem, 0);
		copyConductanceNMDA(netId, lGrpId, dest, &managerRuntimeData, allocateMem, 0);
		copyConductanceGABAa(netId, lGrpId, dest, &managerRuntimeData, allocateMem, 0);
		copyConductanceGABAb(netId, lGrpId, dest, &managerRuntimeData, allocateMem, 0);
	}

	// copying external current needs to be done separately because setExternalCurrent needs to call it, too
	// do it only from host to device
	copyExternalCurrent(netId, lGrpId, dest, allocateMem);
	
	copyNeuronParameters(netId, lGrpId, dest, allocateMem);

	if (sim_with_homeostasis) {
		//Included to enable homeostasis in GPU_MODE.
		// Avg. Firing...
		if(allocateMem)
			dest->avgFiring = new float[length];
		memcpy(&dest->avgFiring[ptrPos], &managerRuntimeData.avgFiring[ptrPos], sizeof(float) * length);
	}
}

/*!
 * \brief this function allocates memory sapce and copies AMPA conductance to it
 *
 * This function:
 * (allocate and) copy gAMPA
 *
 * This funcion is called by copyNeuronState() and fetchConductanceAMPA(). It supports bi-directional copying
 *
 * \param[in] netId the id of a local network, which is the same as the Core (CPU) id
 * \param[in] lGrpId the local group id in a local network, which specifiy the group(s) to be copied
 * \param[in] dest pointer to runtime data desitnation
 * \param[in] src pointer to runtime data source
 * \param[in] allocateMem a flag indicates whether allocating memory space before copy
 * \param[in] destOffset the offset of data destination, which is used in local-to-global copy 
 *
 * \sa copyNeuronState fetchConductanceAMPA
 * \since v3.0
 */
void SNN::copyConductanceAMPA(int netId, int lGrpId, RuntimeData* dest, RuntimeData* src, bool allocateMem, int destOffset) {	
	assert(isSimulationWithCOBA());

	int ptrPos, length;

	if(lGrpId == ALL) {
		ptrPos = 0;
		length = networkConfigs[netId].numNReg;
	} else {
		ptrPos = groupConfigs[netId][lGrpId].lStartN;
		length = groupConfigs[netId][lGrpId].numN;
	}
	assert(length <= networkConfigs[netId].numNReg);
	assert(length > 0);

	//conductance information
	assert(src->gAMPA  != NULL);
	if(allocateMem)
		dest->gAMPA = new float[length];
	memcpy(&dest->gAMPA[ptrPos + destOffset], &src->gAMPA[ptrPos], sizeof(float) * length);
}

/*!
 * \brief this function allocates memory sapce and copies NMDA conductance to it
 *
 * This function:
 * (allocate and) copy gNMDA, gNMDA_r, gNMDA_d
 *
 * This funcion is called by copyNeuronState() and fetchConductanceNMDA(). It supports bi-directional copying
 *
 * \param[in] netId the id of a local network, which is the same as the Core (CPU) id
 * \param[in] lGrpId the local group id in a local network, which specifiy the group(s) to be copied
 * \param[in] dest pointer to runtime data desitnation
 * \param[in] src pointer to runtime data source
 * \param[in] allocateMem a flag indicates whether allocating memory space before copy
 * \param[in] destOffset the offset of data destination, which is used in local-to-global copy 
 *
 * \sa copyNeuronState fetchConductanceNMDA
 * \since v3.0
 */
void SNN::copyConductanceNMDA(int netId, int lGrpId, RuntimeData* dest, RuntimeData* src, bool allocateMem, int destOffset) {
	assert(isSimulationWithCOBA());

	int ptrPos, length;

	if(lGrpId == ALL) {
		ptrPos  = 0;
		length  = networkConfigs[netId].numNReg;
	} else {
		ptrPos  = groupConfigs[netId][lGrpId].lStartN;
		length  = groupConfigs[netId][lGrpId].numN;
	}
	assert(length  <= networkConfigs[netId].numNReg);
	assert(length > 0);

	if (isSimulationWithNMDARise()) {
		assert(src->gNMDA_r != NULL);
		if(allocateMem)
			dest->gNMDA_r = new float[length];
		memcpy(&dest->gNMDA_r[ptrPos], &src->gNMDA_r[ptrPos], sizeof(float) * length);

		assert(src->gNMDA_d != NULL);
		if(allocateMem)
			dest->gNMDA_d = new float[length];
		memcpy(&dest->gNMDA_d[ptrPos], &src->gNMDA_d[ptrPos], sizeof(float) * length);
	} else {
		assert(src->gNMDA != NULL);
		if(allocateMem)
			dest->gNMDA = new float[length];
		memcpy(&dest->gNMDA[ptrPos + destOffset], &src->gNMDA[ptrPos], sizeof(float) * length);
	}
}

/*!
 * \brief this function allocates memory sapce and copies GABAa conductance to it
 *
 * This function:
 * (allocate and) copy gGABAa
 *
 * This funcion is called by copyNeuronState() and fetchConductanceGABAa(). It supports bi-directional copying
 *
 * \param[in] netId the id of a local network, which is the same as the Core (CPU) id
 * \param[in] lGrpId the local group id in a local network, which specifiy the group(s) to be copied
 * \param[in] dest pointer to runtime data desitnation
 * \param[in] src pointer to runtime data source
 * \param[in] allocateMem a flag indicates whether allocating memory space before copy
 * \param[in] destOffset the offset of data destination, which is used in local-to-global copy 
 *
 * \sa copyNeuronState fetchConductanceGABAa
 * \since v3.0
 */
void SNN::copyConductanceGABAa(int netId, int lGrpId, RuntimeData* dest, RuntimeData* src, bool allocateMem, int destOffset) {
	assert(isSimulationWithCOBA());

	int ptrPos, length;

	if(lGrpId == ALL) {
		ptrPos  = 0;
		length  = networkConfigs[netId].numNReg;
	} else {
		ptrPos  = groupConfigs[netId][lGrpId].lStartN;
		length  = groupConfigs[netId][lGrpId].numN;
	}
	assert(length  <= networkConfigs[netId].numNReg);
	assert(length > 0);

	assert(src->gGABAa != NULL);
	if(allocateMem)
		dest->gGABAa = new float[length];
	memcpy(&dest->gGABAa[ptrPos + destOffset], &src->gGABAa[ptrPos], sizeof(float) * length);
}

/*!
 * \brief this function allocates memory sapce and copies GABAb conductance to it
 *
 * This function:
 * (allocate and) copy gGABAb, gGABAb_r, gGABAb_d
 *
 * This funcion is called by copyNeuronState() and fetchConductanceGABAb(). It supports bi-directional copying
 *
 * \param[in] netId the id of a local network, which is the same as the Core (CPU) id
 * \param[in] lGrpId the local group id in a local network, which specifiy the group(s) to be copied
 * \param[in] dest pointer to runtime data desitnation
 * \param[in] src pointer to runtime data source
 * \param[in] allocateMem a flag indicates whether allocating memory space before copy
 * \param[in] destOffset the offset of data destination, which is used in local-to-global copy 
 *
 * \sa copyNeuronState fetchConductanceGABAb
 * \since v3.0
 */
void SNN::copyConductanceGABAb(int netId, int lGrpId, RuntimeData* dest, RuntimeData* src, bool allocateMem, int destOffset) {
	assert(isSimulationWithCOBA());

	int ptrPos, length;

	if(lGrpId == ALL) {
		ptrPos  = 0;
		length  = networkConfigs[netId].numNReg;
	} else {
		ptrPos  = groupConfigs[netId][lGrpId].lStartN;
		length  = groupConfigs[netId][lGrpId].numN;
	}
	assert(length <= networkConfigs[netId].numNReg);
	assert(length > 0);

	if (isSimulationWithGABAbRise()) {
		assert(src->gGABAb_r != NULL);
		if(allocateMem)
			dest->gGABAb_r = new float[length];
		memcpy(&dest->gGABAb_r[ptrPos], &src->gGABAb_r[ptrPos], sizeof(float) * length);

		assert(src->gGABAb_d != NULL);
		if(allocateMem) 
			dest->gGABAb_d = new float[length];
		memcpy(&dest->gGABAb_d[ptrPos], &src->gGABAb_d[ptrPos], sizeof(float) * length);
	} else {
		assert(src->gGABAb != NULL);
		if(allocateMem)
			dest->gGABAb = new float[length];
		memcpy(&dest->gGABAb[ptrPos + destOffset], &src->gGABAb[ptrPos], sizeof(float) * length);
	}
}

/*!
 * \brief this function allocates memory sapce and copies external current to it
 *
 * This function:

 * (allocate and) copy extCurrent
 *
 * This funcion is called by copyNeuronState() and setExternalCurrent. Only host-to-divice copy is required
 *
 * \param[in] netId the id of a local network, which is the same as the Core (CPU) id
 * \param[in] lGrpId the local group id in a local network, which specifiy the group(s) to be copied
 * \param[in] dest pointer to runtime data desitnation
 * \param[in] allocateMem a flag indicates whether allocating memory space before copying
 *
 * \sa allocateSNN_CPU fetchSTPState
 * \since v3.0
 */
void SNN::copyExternalCurrent(int netId, int lGrpId, RuntimeData* dest, bool allocateMem) {	
	int posN, lengthN;

	if(lGrpId == ALL) {
		posN  = 0;
		lengthN  = networkConfigs[netId].numNReg;
	} else {
		assert(lGrpId >= 0);
		posN = groupConfigs[netId][lGrpId].lStartN;
		lengthN = groupConfigs[netId][lGrpId].numN;
	}
	assert(lengthN >= 0 && lengthN <= networkConfigs[netId].numNReg); // assert NOT poisson neurons

	KERNEL_DEBUG("copyExternalCurrent: lGrpId=%d, ptrPos=%d, length=%d, allocate=%s", lGrpId, posN, lengthN, allocateMem?"y":"n");

	if(allocateMem)
		dest->extCurrent = new float[lengthN];
	memcpy(&(dest->extCurrent[posN]), &(managerRuntimeData.extCurrent[posN]), sizeof(float) * lengthN);
}

/*!
 * \brief this function allocates memory sapce and copies neural parameters to it
 *
 * This function:
 * (allocate and) copy Izh_a, Izh_b, Izh_c, Izh_d
 * initialize baseFiringInv
 * (allocate and) copy baseFiring, baseFiringInv
 *
 * This funcion is only called by copyNeuronState(). Only copying direction from host to device is required.
 *
 * \param[in] netId the id of a local network, which is the same as the Core (CPU) id
 * \param[in] lGrpId the local group id in a local network, which specifiy the group(s) to be copied
 * \param[in] dest pointer to runtime data desitnation
 * \param[in] allocateMem a flag indicates whether allocating memory space before copying
 *
 * \sa copyNeuronState
 * \since v3.0
 */
void SNN::copyNeuronParameters(int netId, int lGrpId, RuntimeData* dest, bool allocateMem) {
	int ptrPos, length;

	// when allocating we are allocating the memory.. we need to do it completely... to avoid memory fragmentation..
	if (allocateMem) {
		assert(lGrpId == ALL);
		assert(dest->Izh_a == NULL);
		assert(dest->Izh_b == NULL);
		assert(dest->Izh_c == NULL);
		assert(dest->Izh_d == NULL);
	}

	if(lGrpId == ALL) {
		ptrPos = 0;
		length = networkConfigs[netId].numNReg;
	}
	else {
		ptrPos = groupConfigs[netId][lGrpId].lStartN;
		length = groupConfigs[netId][lGrpId].numN;
	}

	if(allocateMem)
		dest->Izh_a = new float[length];
	memcpy(&dest->Izh_a[ptrPos], &(managerRuntimeData.Izh_a[ptrPos]), sizeof(float) * length);

	if(allocateMem)
		dest->Izh_b = new float[length];
	memcpy(&dest->Izh_b[ptrPos], &(managerRuntimeData.Izh_b[ptrPos]), sizeof(float) * length);

	if(allocateMem)
		dest->Izh_c = new float[length];
	memcpy(&dest->Izh_c[ptrPos], &(managerRuntimeData.Izh_c[ptrPos]), sizeof(float) * length);

	if(allocateMem)
		dest->Izh_d = new float[length];
	memcpy(&dest->Izh_d[ptrPos], &(managerRuntimeData.Izh_d[ptrPos]), sizeof(float) * length);

	// pre-compute baseFiringInv for fast computation on GPUs.
	if (sim_with_homeostasis) {
		float* baseFiringInv = new float[length];
		for(int nid = 0; nid < length; nid++) {
			if (managerRuntimeData.baseFiring[nid] != 0.0f)
				baseFiringInv[nid] = 1.0f / managerRuntimeData.baseFiring[ptrPos + nid];
			else
				baseFiringInv[nid] = 0.0;
		}

		if(allocateMem)
			dest->baseFiringInv = new float[length];
		memcpy(&dest->baseFiringInv[ptrPos], baseFiringInv, sizeof(float) * length);

		if(allocateMem)
			dest->baseFiring = new float[length];
		memcpy(&dest->baseFiring[ptrPos], managerRuntimeData.baseFiring, sizeof(float) * length);

		delete [] baseFiringInv;
	}
}

/*!
 * \brief this function allocates memory sapce and copies short-term plasticity (STP) state to it
 *
 * This function:
 * initialize STP_Pitch
 * (allocate and) copy stpu, stpx
 *
 * This funcion is called by allocateSNN_GPU() and fetchSTPState(). It supports bi-directional copying
 *
 * \param[in] netId the id of a local network, which is the same as the Core (CPU) id
 * \param[in] lGrpId the local group id in a local network, which specifiy the group(s) to be copied
 * \param[in] dest pointer to runtime data desitnation
 * \param[in] src pointer to runtime data source
 * \param[in] allocateMem a flag indicates whether allocating memory space before copying
 *
 * \sa allocateSNN_CPU fetchSTPState
 * \since v3.0
 */
void SNN::copySTPState(int netId, int lGrpId, RuntimeData* dest, RuntimeData* src, bool allocateMem) {
	// STP feature is optional, do addtional check for memory space
	if(allocateMem) {
		assert(dest->stpu == NULL);
		assert(dest->stpx == NULL);
	} else {
		assert(dest->stpu != NULL);
		assert(dest->stpx != NULL);
	}
	assert(src->stpu != NULL); assert(src->stpx != NULL);

	if(allocateMem)
		dest->stpu = new float[networkConfigs[netId].numN * networkConfigs[netId].maxDelay + 1];
	memcpy(dest->stpu, src->stpu, sizeof(float) * networkConfigs[netId].numN * networkConfigs[netId].maxDelay + 1);

	if(allocateMem)
		dest->stpx = new float[networkConfigs[netId].numN * networkConfigs[netId].maxDelay + 1];
	memcpy(dest->stpx, src->stpx, sizeof(float) * networkConfigs[netId].numN * networkConfigs[netId].maxDelay + 1);
}

// ToDo: move grpDA(5HT, ACh, NE)Buffer to copyAuxiliaryData
/*!
 * \brief this function allocates memory sapce and copies variables related to group state to it
 *
 * This function:
 * (allocate and) copy grpDA, grp5HT, grpACh, grpNE, grpDABuffer, grp5HTBuffer, grpAChBuffer, grpNEBuffer
 *
 * This funcion is called by allocateSNN_GPU() and fetchGroupState(). It supports bi-directional copying
 *
 * \param[in] netId the id of a local network, which is the same as the Core (CPU) id
 * \param[in] lGrpId the local group id in a local network, which specifiy the group(s) to be copied
 * \param[in] dest pointer to runtime data desitnation
 * \param[in] src pointer to runtime data source
 * \param[in] allocateMem a flag indicates whether allocating memory space before copying
 *
 * \sa allocateSNN_GPU fetchGroupState
 * \since v3.0
 */
void SNN::copyGroupState(int netId, int lGrpId, RuntimeData* dest, RuntimeData* src, bool allocateMem) {
	if (allocateMem) {
		assert(dest->memType == CPU_MODE && !dest->allocated);
		dest->grpDA = new float[networkConfigs[netId].numGroups]; 
		dest->grp5HT = new float[networkConfigs[netId].numGroups]; 
		dest->grpACh = new float[networkConfigs[netId].numGroups]; 
		dest->grpNE = new float[networkConfigs[netId].numGroups];
	}
	memcpy(dest->grpDA, src->grpDA, sizeof(float) * networkConfigs[netId].numGroups);
	memcpy(dest->grp5HT, src->grp5HT, sizeof(float) * networkConfigs[netId].numGroups);
	memcpy(dest->grpACh, src->grpACh, sizeof(float) * networkConfigs[netId].numGroups);
	memcpy(dest->grpNE, src->grpNE, sizeof(float) * networkConfigs[netId].numGroups);

	if (lGrpId == ALL) {
		if (allocateMem) {
			assert(dest->memType == CPU_MODE && !dest->allocated);
			dest->grpDABuffer = new float[1000 * networkConfigs[netId].numGroups]; 
			dest->grp5HTBuffer = new float[1000 * networkConfigs[netId].numGroups]; 
			dest->grpAChBuffer = new float[1000 * networkConfigs[netId].numGroups]; 
			dest->grpNEBuffer = new float[1000 * networkConfigs[netId].numGroups];
		}
		memcpy(dest->grpDABuffer, src->grpDABuffer, sizeof(float) * 1000 * networkConfigs[netId].numGroups);
		memcpy(dest->grp5HTBuffer, src->grp5HTBuffer, sizeof(float) * 1000 * networkConfigs[netId].numGroups);
		memcpy(dest->grpAChBuffer, src->grpAChBuffer, sizeof(float) * 1000 * networkConfigs[netId].numGroups);
		memcpy(dest->grpNEBuffer, src->grpNEBuffer, sizeof(float) * 1000 * networkConfigs[netId].numGroups);
	} else {
		assert(!allocateMem);
		memcpy(&dest->grpDABuffer[lGrpId * 1000], &src->grpDABuffer[lGrpId * 1000], sizeof(float) * 1000);
		memcpy(&dest->grp5HTBuffer[lGrpId * 1000], &src->grp5HTBuffer[lGrpId * 1000], sizeof(float) * 1000);
		memcpy(&dest->grpAChBuffer[lGrpId * 1000], &src->grpAChBuffer[lGrpId * 1000], sizeof(float) * 1000);
		memcpy(&dest->grpNEBuffer[lGrpId * 1000], &src->grpNEBuffer[lGrpId * 1000], sizeof(float) * 1000);
	}
}

/*!
 * \brief this function allocates memory sapce and copies auxiliary runtime data to it
 *
 * This function:
 * (allocate and) reset spikeGenBits, poissonFireRate
 * initialize I_setLength, I_setPitch; (allocate and) reset I_set
 * (allocate and) copy synSpikeTime, lastSpikeTime
 * (allocate and) copy nSpikeCnt
 * (allocate and) copy grpIds, connIdsPreIdx
 * (allocate and) copy timeTableD1, timeTableD2
 * (allocate and) copy firingTableD1, firingTableD2
 * This funcion is only called by allocateSNN_GPU. Therefore, only copying direction from host to device is required
 *
 * \param[in] netId the id of local network, which is the same as Core (CPU) id
 * \param[in] dest pointer to runtime data desitnation
 * \param[in] allocateMem a flag indicates whether allocating memory space before copying
 *
 * \sa allocateSNN_GPU
 * \since v4.0
 */
void SNN::copyAuxiliaryData(int netId, int lGrpId, RuntimeData* dest, bool allocateMem) {
	assert(networkConfigs[netId].numN > 0);

	if(allocateMem)
		dest->spikeGenBits = new unsigned int[networkConfigs[netId].numNSpikeGen / 32 + 1];
	memset(dest->spikeGenBits, 0, sizeof(int) * (networkConfigs[netId].numNSpikeGen / 32 + 1));

	// allocate the poisson neuron poissonFireRate
	if(allocateMem)
		dest->poissonFireRate = new float[networkConfigs[netId].numNPois];
	memset(dest->poissonFireRate, 0, sizeof(float) * networkConfigs[netId].numNPois);

	// synaptic auxiliary data
	// I_set: a bit vector indicates which synapse got a spike
	if(allocateMem) {
		networkConfigs[netId].I_setLength = ceil(((networkConfigs[netId].maxNumPreSynN) / 32.0f));
		dest->I_set = new int[networkConfigs[netId].numNReg * networkConfigs[netId].I_setLength];
	}
	assert(networkConfigs[netId].maxNumPreSynN >= 0);
	memset(dest->I_set, 0, sizeof(int) * networkConfigs[netId].numNReg * networkConfigs[netId].I_setLength);

	// synSpikeTime: an array indicates the last time when a synapse got a spike
	if(allocateMem)
		dest->synSpikeTime = new int[networkConfigs[netId].numPreSynNet];
	memcpy(dest->synSpikeTime, managerRuntimeData.synSpikeTime, sizeof(int) * networkConfigs[netId].numPreSynNet);

	// neural auxiliary data
	// lastSpikeTime: an array indicates the last time of a neuron emitting a spike
	if (!sim_with_fixedwts) {
		// neuron firing time
		if(allocateMem)
			dest->lastSpikeTime = new int[networkConfigs[netId].numNAssigned];
		memcpy(dest->lastSpikeTime, managerRuntimeData.lastSpikeTime, sizeof(int) * networkConfigs[netId].numNAssigned);
	}

	// auxiliary data for recording spike count of each neuron
	copyNeuronSpikeCount(netId, lGrpId, dest, &managerRuntimeData, true, 0);

	// quick lookup array for local group ids
	if(allocateMem)
		dest->grpIds = new short int[networkConfigs[netId].numNAssigned];
	memcpy(dest->grpIds, managerRuntimeData.grpIds, sizeof(short int) * networkConfigs[netId].numNAssigned);

	// quick lookup array for conn ids
	if(allocateMem)
		dest->connIdsPreIdx = new short int[networkConfigs[netId].numPreSynNet];
	memcpy(dest->connIdsPreIdx, managerRuntimeData.connIdsPreIdx, sizeof(short int) * networkConfigs[netId].numPreSynNet);

	// reset variable related to spike count
	// Note: the GPU counterpart is not required to do this
	dest->spikeCountSec = 0;
	dest->spikeCountD1Sec = 0;
	dest->spikeCountD2Sec = 0;
	dest->spikeCountLastSecLeftD2 = 0;
	dest->spikeCount = 0;
	dest->spikeCountD1 = 0;
	dest->spikeCountD2 = 0;
	dest->nPoissonSpikes = 0;
	dest->spikeCountExtRxD1 = 0;
	dest->spikeCountExtRxD2 = 0;
	
	// time talbe
	// Note: the GPU counterpart is not required to do this
	if (allocateMem) {
		assert(dest->timeTableD1 == NULL);
		assert(dest->timeTableD2 == NULL);
	}

	if (allocateMem)
		dest->timeTableD1 = new unsigned int[TIMING_COUNT];
	memset(dest->timeTableD1, 0, sizeof(int) * TIMING_COUNT);

	if (allocateMem)
		dest->timeTableD2 = new unsigned int[TIMING_COUNT];
	memset(dest->timeTableD2, 0, sizeof(int) * TIMING_COUNT);

	// firing table
	if (allocateMem) {
		assert(dest->firingTableD1 == NULL);
		assert(dest->firingTableD2 == NULL);
	}

	// allocate 1ms firing table
	if (allocateMem)
		dest->firingTableD1 = new int[networkConfigs[netId].maxSpikesD1];
	if (networkConfigs[netId].maxSpikesD1 > 0)
		memcpy(dest->firingTableD1, managerRuntimeData.firingTableD1, sizeof(int) * networkConfigs[netId].maxSpikesD1);

	// allocate 2+ms firing table
	if(allocateMem)
		dest->firingTableD2 = new int[networkConfigs[netId].maxSpikesD2];
	if (networkConfigs[netId].maxSpikesD2 > 0)
		memcpy(dest->firingTableD2, managerRuntimeData.firingTableD2, sizeof(int) * networkConfigs[netId].maxSpikesD2);

	// allocate external 1ms firing table
	if (allocateMem) {
		dest->extFiringTableD1 = new int*[networkConfigs[netId].numGroups];
		memset(dest->extFiringTableD1, 0 /* NULL */, sizeof(int*) * networkConfigs[netId].numGroups);
		for (int lGrpId = 0; lGrpId < networkConfigs[netId].numGroups; lGrpId++) {
			if (groupConfigs[netId][lGrpId].hasExternalConnect) {
				dest->extFiringTableD1[lGrpId]= new int[groupConfigs[netId][lGrpId].numN * NEURON_MAX_FIRING_RATE];
				memset(dest->extFiringTableD1[lGrpId], 0 , sizeof(int) * groupConfigs[netId][lGrpId].numN * NEURON_MAX_FIRING_RATE);
			}
		}
	}

	// allocate external 2+ms firing table
	if (allocateMem) {
		dest->extFiringTableD2 = new int*[networkConfigs[netId].numGroups];
		memset(dest->extFiringTableD2, 0 /* NULL */, sizeof(int*) * networkConfigs[netId].numGroups);
		for (int lGrpId = 0; lGrpId < networkConfigs[netId].numGroups; lGrpId++) {
			if (groupConfigs[netId][lGrpId].hasExternalConnect) {
				dest->extFiringTableD2[lGrpId] = new int[groupConfigs[netId][lGrpId].numN * NEURON_MAX_FIRING_RATE];
				memset(dest->extFiringTableD2[lGrpId], 0 , sizeof(int) * groupConfigs[netId][lGrpId].numN * NEURON_MAX_FIRING_RATE);
			}
		}
	}

	// allocate external 1ms firing table index
	if (allocateMem)
		dest->extFiringTableEndIdxD1 = new int[networkConfigs[netId].numGroups];
	memset(dest->extFiringTableEndIdxD1, 0, sizeof(int) * networkConfigs[netId].numGroups);


	// allocate external 2+ms firing table index
	if (allocateMem)
		dest->extFiringTableEndIdxD2 = new int[networkConfigs[netId].numGroups];
	memset(dest->extFiringTableEndIdxD2, 0, sizeof(int) * networkConfigs[netId].numGroups);
}

/*!
 * \brief this function allocates memory sapce and copies the spike count of each neuron to it
 *
 * This function:
 * (allocate and) copy nSpikeCnt
 *
 * This funcion is called by copyAuxiliaryData() and fetchNeuronSpikeCount(). It supports bi-directional copying
 *
 * \param[in] netId the id of a local network, which is the same as the Core (CPU) id
 * \param[in] lGrpId the local group id in a local network, which specifiy the group(s) to be copied
 * \param[in] dest pointer to runtime data desitnation
 * \param[in] src pointer to runtime data source
 * \param[in] allocateMem a flag indicates whether allocating memory space before copy
 * \param[in] destOffset the offset of data destination, which is used in local-to-global copy 
 *
 * \sa copyAuxiliaryData fetchNeuronSpikeCount
 * \since v4.0
 */
void SNN::copyNeuronSpikeCount(int netId, int lGrpId, RuntimeData* dest, RuntimeData* src, bool allocateMem, int destOffset) {
	int posN, lengthN;

	if(lGrpId == ALL) {
		posN = 0;
		lengthN = networkConfigs[netId].numN;
	} else {
		posN = groupConfigs[netId][lGrpId].lStartN;
		lengthN = groupConfigs[netId][lGrpId].numN;
	}
	assert(lengthN > 0 && lengthN <= networkConfigs[netId].numN);

	// spike count information
	if(allocateMem)
		dest->nSpikeCnt = new int[lengthN];
	memcpy(&dest->nSpikeCnt[posN + destOffset], &src->nSpikeCnt[posN], sizeof(int) * lengthN);
}

void SNN::assignPoissonFiringRate() {
	for (int netId = 0; netId < MAX_NET_PER_SNN; netId++) {
		if (!groupPartitionLists[netId].empty()) {
			for (int lGrpId = 0; lGrpId < networkConfigs[netId].numGroups; lGrpId++) {
				// given group of neurons belong to the poisson group....
				if (groupConfigs[netId][lGrpId].isSpikeGenerator) {
					int lNId = groupConfigs[netId][lGrpId].lStartN;
					int gGrpId = groupConfigs[netId][lGrpId].gGrpId;
					PoissonRate* rate = groupConfigMDMap[gGrpId].ratePtr;

					// if spikeGenFunc group does not have a Poisson pointer, skip
					if (groupConfigMap[gGrpId].spikeGenFunc || rate == NULL)
						continue;

					assert(gpuRuntimeData[netId].poissonFireRate != NULL);
					assert(rate->isOnGPU() == false);
					// rates allocated on CPU
					memcpy(&cpuRuntimeData[netId].poissonFireRate[lNId - networkConfigs[netId].numNReg], rate->getRatePtrCPU(),
							sizeof(float) * rate->getNumNeurons());
				}
			}
		}
	}
}

void SNN::deleteObjects_CPU() {
	// ToDo: Add memory deallocation
}
// ToDo: remove this, make possion spike generation consistent with GPU_MODE
// will be used in generateSpikesFromRate
// The time between each pair of consecutive events has an exponential distribution with parameter \lambda and
// each of these ISI values is assumed to be independent of other ISI values.
// What follows a Poisson distribution is the actual number of spikes sent during a certain interval.
//int SNN::poissonSpike(int currTime, float frate, int refractPeriod) {
//	// refractory period must be 1 or greater, 0 means could have multiple spikes specified at the same time.
//	assert(refractPeriod>0);
//	assert(frate>=0.0f);
//
//	bool done = false;
//	int nextTime = 0;
//	while (!done) {
//		// A Poisson process will always generate inter-spike-interval (ISI) values from an exponential distribution.
//		float randVal = drand48();
//		int tmpVal  = -log(randVal)/frate;
//
//		// add new ISI to current time
//		// this might be faster than keeping currTime fixed until drand48() returns a large enough value for the ISI
//		nextTime = currTime + tmpVal;
//
//		// reject new firing time if ISI is smaller than refractory period
//		if ((nextTime - currTime) >= refractPeriod)
//			done = true;
//	}
//
//	assert(nextTime != 0);
//	return nextTime;
//}

// FIXME: wrong to use groupConfigs[0]
// ToDo: remove this, make possion spike generation consistent with GPU_MODE
//void SNN::generateSpikesFromRate(int gGrpId) {
//	bool done;
//	PoissonRate* rate = groupConfigMDMap[gGrpId].ratePtr;
//	float refPeriod = groupConfigMDMap[gGrpId].refractPeriod;
//	int timeSlice   = groupConfigMDMap[gGrpId].currTimeSlice;
//	int currTime = simTime;
//	int spikeCnt = 0;
//
//	if (rate == NULL)
//		return;
//
//	if (rate->isOnGPU()) {
//		KERNEL_ERROR("Specifying rates on the GPU but using the CPU SNN is not supported.");
//		exitSimulation(1);
//	}
//
//	const int nNeur = rate->getNumNeurons();
//	if (nNeur != groupConfigMap[gGrpId].numN) {
//		KERNEL_ERROR("Length of PoissonRate array (%d) did not match number of neurons (%d) for group %d(%s).",
//			nNeur, groupConfigMap[gGrpId].numN, gGrpId, groupConfigMap[gGrpId].grpName.c_str());
//		exitSimulation(1);
//	}
//
//	for (int neurId=0; neurId<nNeur; neurId++) {
//		float frate = rate->getRate(neurId);
//
//		// start the time from the last time it spiked, that way we can ensure that the refractory period is maintained
//		int nextTime = managerRuntimeData.lastSpikeTime[groupConfigMDMap[gGrpId].gStartN + neurId];
//		if (nextTime == MAX_SIMULATION_TIME)
//			nextTime = 0;
//
//		done = false;
//		while (!done && frate>0) {
//			nextTime = poissonSpike(nextTime, frate/1000.0, refPeriod);
//			// found a valid timeSlice
//			if (nextTime < (currTime+timeSlice)) {
//				if (nextTime >= currTime) {
////					int nid = groupConfigs[0][grpId].StartN+cnt;
//					spikeBuf->schedule(groupConfigMDMap[gGrpId].gStartN + neurId, nextTime-currTime);
//					spikeCnt++;
//				}
//			}
//			else {
//				done=true;
//			}
//		}
//	}
//}